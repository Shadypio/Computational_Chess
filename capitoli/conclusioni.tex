\chapter{Conclusioni e sviluppi futuri} %\label{1cap:spinta_laterale}
% [titolo ridotto se non ci dovesse stare] {titolo completo}
%


\begin{citazione}
    Questo capitolo vuole riassumere il lavoro svolto e analizzarne da un punto di vista critico i risultati ottenuti. Vengono in seguito proposti diversi punti di vista e spunti di riflessione che potrebbero essere utilizzati in futuro per migliorare e rifinire tutto il lavoro svolto in entrambi i moduli.
\end{citazione}

\section{Riflessioni sui risultati complessivi}
Gli obiettivi del presente lavoro di tesi sono stati raggiunti in maniera conforme a quanto scritto nel Capitolo \ref{cap: introduzione}. L'implementazione \textit{from scratch} del primo modulo è stata di grande aiuto per comprendere non solo la complessità di un algoritmo che ricercasse le mosse disponibili ma anche l'importanza nel considerare una funzione obiettivo che valuti lo stato della scacchiera. Inoltre, un lavoro di questo tipo ha stimolato l'idea per la creazione di Caissa, un modello di rete neurale che ha imparato a giocare a scacchi nella sua fase di addestramento. Di seguito vengono esaminati nel dettaglio i punti di forza e di debolezza di ciascuno dei due moduli sviluppati.
\subsection{Primo Modulo}
Sebbene da un lato la "semplice" e visivamente intuitiva implementazione sia risultata favorevole per l'utilizzo di algoritmi di ricerca delle mosse, dall'altro l'analisi delle prestazioni trattata nel paragrafo \ref{cap: Analisi} ha lasciato emergere dei limiti in termini di tempi di esecuzione degli algoritmi utilizzati. 
\subsubsection{Threading}
Una possibile soluzione al problema riguarderebbe l'utilizzo di diversi \textbf{thread}, ciascuno dedicato alla visita di porzioni dell'albero da eseguire in tempi significativamente contenuti. Il main thread, dopo aver atteso le esecuzioni dei diversi thread, raccoglierebbe le informazioni ricevute e le elaborerebbe per decidere la mossa migliore da giocare data la situazione sulla scacchiera. L'obiettivo di questo approccio sarebbe volto dunque a ridurre i tempi di esecuzione del programma, e non la complessità temporale che è invece intrinseca all'algoritmo.
\subsubsection{Negascout}
Benché la tecnica della potatura alfa-beta abbia ridotto i tempi di visita degli alberi di gioco, si potrebbe optare per la scelta di un algoritmo che in alcuni casi possa addirittura essere più veloce della potatura stessa, a condizione che l'albero sia accuratamente ordinato. Un corretto ordinamento porterebbe l'algoritmo Negascout ad assumere che il nodo corrente sia già quello migliore, producendo un numero maggiore di tagli della potatura alfa-beta. Se venisse visitato un nodo effettivamente migliore di quello considerato si dovrebbe ripercorrere una ricerca normale alfa-beta su quel nodo; un ordinamento scorretto dell'albero porterebbe dunque a tempi di esecuzione più elevati rispetto a un comune algoritmo con potatura alfa-beta. L'adozione dell'algoritmo Negascout nel gioco degli scacchi porterebbe a un incremento delle prestazioni di circa il 10\%\cite{reinefeld1983improvement} rispetto alla strategia utilizzata nel modulo implementato.
\subsection{Secondo Modulo}
L'addestramento di Caissa effettuato con le metodologie descritte nel paragrafo \ref{cap: caissa} ha portato alla creazione di un modello dalle performance sportive non del tutto soddisfacenti. Tali limiti vanno probabilmente ricercati nelle tecniche utilizzate per l'addestramento della rete neurale. Con tecniche di apprendimento supervisionato come quelle utilizzate viene fornito un insieme di dati già etichettati e pensati proprio per l'addestramento, ma la natura stessa dei dati rimane pressoché invariata. Di seguito viene proposta un'alternativa alla metodologia di apprendimento utilizzata, che tenga conto di una diversa \textbf{funzione} che entra in gioco nell'elaborazione dei dati forniti in input alla rete neurale.
\subsubsection{Apprendimento per rinforzo (\textit{reinforcement learning})}
Una buona scelta del tipo di addestramento della rete neurale potrebbe ricadere nella metodologia di apprendimento per \textbf{rinforzo}. In questo tipo di addestramento l'agente viene addestrato per effettuare delle decisioni in un determinato ambiente in base ai dati percepiti. L'azione effettuata è poi accompagnata da un valore di \textbf{ricompensa} proporzionato alla qualità dell'azione svolta, per incoraggiare oppure scoraggiare quella tipologia di azioni in futuro. Benché esistano diverse \textbf{funzioni di rinforzo}, esse possono essere ricondotte alla formula: 
\begin{equation}
    v_{t+1} = (1-\alpha)v_t(s) + \alpha\Delta_{t+1}
\end{equation}
dove $0<\alpha\leq1$ e $\Delta_{t+1}$ è la ricompensa per una data azione. Il miglioramento delle azioni è effettuato in modo da massimizzare il valore della \textbf{funzione di valore d'azione}, cioè di quella funzione che valuta, appunto, l'azione eseguita in uno stato.
Un noto algoritmo di apprendimento per rinforzo è il \textbf{Q-learning}: viene costruita una Q-table le cui righe e colonne rappresentano rispettivamente i possibili stati dell'ambiente e le ricompense ottenute in quello stato dopo un'interazione con esso da parte del modello. In questo modo, una volta riempita la tabella, dato uno determinato stato l'agente conosce già l'azione da eseguire per ottenere il valore di ricompensa più alto. Benché il Q-learning si presti bene alla risoluzione di problemi dalla complessità contenuta, nel gioco degli scacchi sarebbe improponibile cercare di costruire (e completare!) una tabella che tenga traccia di tutti i possibili stati della scacchiera. Tuttavia, potrebbe rivelarsi utile sfruttare i tanti livelli intermedi di una rete neurale profonda per costruire un algoritmo di \textit{deep reinforcement learning} che tenga conto dei numerosi stati possibili della scacchiera e che possa dunque pesare il valore di ricompensa adattandolo alle circostanze.


\section{Integrazione dei due moduli}
La realizzazione di tutto il lavoro stimolerebbe un'integrazione dei due moduli in maniera complementare. Potrebbe essere utile sfruttare l'implementazione del primo modulo per consentire a un giocatore umano di giocare contro il computer e conservare in un file le mosse e gli esiti delle partite. Questo file, se adeguatamente grande, potrebbe essere messo a disposizione in futuro per l'addestramento di una rete neurale (adottando le convenzioni del caso). Un lavoro del genere porterebbe dunque alla creazione di un motore scacchistico dallo stile di gioco affine a quello del giocatore, che potrebbe sfruttare il motore come strumento di \textit{Self-Play} per migliorare il proprio punteggio ELO. 
\newpage
